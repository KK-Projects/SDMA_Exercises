# SDMA TP4
# Ahmed-Karim BERRADA
# Karl BOU ABBOUD
# Marouane YALA

# Décomposition en valeurs singulières & Analyse en composantes principales


# EXERCICE 1 : DECOMPOSITION EN VALEURS SINGULIERES

n = 50
p = 5

X = matrix(rnorm(p*n), nrow=n, ncol=p)
S = cov(X)

# Les coefficients diagonaux de S sont plus significatif que les coefficients non-diagonaux
# à cause de l'indépendance des variables Xj

# La décomposition en valeures singulieres consiste à decomposer une matrice en un produit
# U . Sigma . tV avec U et V sont orthogonales, respectivement matrice de passage et de sortie et
  # Sigma est une matrice diagonale contenant les valeures singulières de S (racine des valeures propres de tS.S)

svd_ <- svd(S)
U <- svd_$u
V <- svd_$v
Sigma <- diag(svd_$d)

# S est symÃ©trique, on a U = V et les valeures propres de tS.S sont les valeures propres de S
# Sigma est donc une dÃ©composition en valeures propres.
# On compare U et V:

norm(U - V, type="F") # 1e-15
norm(U - V, type="2")  # 1e-15

sum(Sigma) - sum(diag(S)) # ~ 8e-16 les traces sont égales 

S2 <- U %*% Sigma %*% t(V)
norm(S2 - S, type="F") # ~3e-15
# Les deux matrices sont égales


# Analyse en composantes principales

# Le fichier contient les caractÃ©ristiques de plusieurs marques de voitures
# les caractÃ©ristiques sont: cylindre; puissance; vitess; poids; longeur; largeur
# les donnÃ©es sont d'ordre de grandeur assez diffÃ©rents

X <- read.table("cardata.txt", header=T, row.names=1, sep=";")
ncol(X) # 6 colonnes
nrow(X) # 24 lignes
plot(X) 
# Ce graphique reprÃ©sente les graphes des variables de nos donnÃ©es prise deux Ã  deux.
# si le graphe ne semble pas dispersÃ© les variables sont sÃ»rement corrÃ©lÃ©s

library("corrplot")

cor(X)
corrplot(cor(X))
# Il y a de fortes correlation, la cylindre avec la puissance ou la longeur et le poids par exemple


# ACP

res <- prcomp(X, scale.=TRUE)
attributes(res)
# les attributs sont:
# sdev: la variance des composantes principales ordonnÃ©es de maniÃ¨res dÃ©croissante
# cela correspond aussi Ã  la racine carrÃ©es des valeures propres de la matrice de covariance
# rotation: la matrice des vecteurs propres
# x: coordonnÃ©es des vecteurs de l'ancienne base dans la nouvelle base
# center, scale: moyennes et variance utilisÃ©es pour normaliser les variables

valpr <- res$sdev**2
plot(valpr, main="Valeures propres par ordre dÃ©croissant")
cumsum(valpr) / length(valpr)
# pour expliquer 95% de la covariance les trois premiÃ¨res composantes suffisent
# les quatres premiÃ¨res pour 98%

res$rotation
# pour le premier axe toutes les coordonnÃ©es sont prise en compte positivement avec un facteur 0.4
# pour le second axe cylind, puiss, vit sont pris en compte positivement, les autres nÃ©gativement,
# la cylindrÃ©e Ã  une importance plus faible que les autres variables
# pour le troisieme axe, la cylindres, la pusisance et le poids ont des contributions nÃ©gatives
# la contribution de la puissance et de longeur sont trÃ¨s faibles
# pour le quatriÃ¨me axe la vitesse et la longeur ont une contribution nÃ©gative
# la contribution de la cylindrÃ©e est faible et celle de la longeur est importante

res2 <- dudi.pca(X, scannf=F, nf=6)

biplot(res,choices=1:2)
s.corcircle(res2$co[,1:2])

biplot(res,choices=2:3)
s.corcircle(res2$co[,2:3])

# Dans le premier graphe le premier axe indique les voitures qui ont des caractÃ©ristiques importante dans tous les domaines
# ce sont les grosses voitures, ces voitures sont reprÃ©sentÃ©s Ã  droite sur le graphe.
# le deuxiÃ¨me axe met en opposition la cylindre, la puissance et la vitesse aux autres coordonnÃ©es
# il s'agit de diffÃ©rencier le moteur de la taille de la voiture
# Pour le deuxiÃ¨me graphe on compare cette fois-ci le poids ainsi que la puissance du moteur aux autres caractÃ©ristiques

## Classification non supervisÃ©e

res1 <- kmeans(X, 1)
res1$centers # CoordonnÃ©es des centres
res1$cluster # Classe de chaque point
res1$withinss # vector contenant la somme des distances au carrÃ© de chaque center aux points de son cluster
res1$betweenss # somme des distances au carrÃ© de chaque voitures qui ne font pas partie du mÃªme cluster
# la classification pour k=1 Ã  peu de sens car toute les voitures appartiennent au mÃªme cluster

res2 <- kmeans(X,2)
res2$centers
res2$cluster
res2$withinss
res2$betweenss
# On a ici deux clusters pour les voitures puissantes et imposantes et les voitures plus lÃ¨gere 

res3 <- kmeans(X,3)
res3$centers
res3$cluster
res3$withinss
res3$betweenss
# MÃªme chose que prÃ©cedemment avec une classe intermÃ©diaire

res4 <- kmeans(X,4)
res4$centers
res4$cluster
res4$withinss
res4$betweenss
# Une classe intermÃ©diaire en plus

## CaractÃ¨res manuscrits

mImage = function(x) {
  img <- matrix(x,16,16,byrow = T)
  image(img, axes=FALSE, col=gray(0:255/255))
}

indices <- sample(nrow(d3), 1000, replace=FALSE)
d3train <- d3[indices,]
d3test <- d3[-indices,]
d8train <- d8[indices,]
d8test <- d8[-indices,]

# 3 et 8 moyen
mean3 <- colMeans(d3train)
mean8 <- colMeans(d8train)

mImage(mean3)
mImage(mean8)

d3train_s <- scale(d3train, center=T, scale=T)
var3 <- attr(d3train,"scaled:center")

d8train_s <- scale(d8train, center=T, scale=T)
var8 <- attr(d8train,"scaled:center")

eigen3 <- eigen(cov(d3train_s))
eigenval_3 <- eigen3$values
eigenvec_3 <- eigen3$vectors

eigen8 <- eigen(cov(d8train_s))
eigenval_8 <- eigen8$values
eigenvec_8 <- eigen8$vectors

# les valeures singulieres sont Ã©gales aux racines des valeures propres de la matrice de covariance

mod31 <- eigenvec_3[,1] %*% t(eigenvec_3[,1])
mod32 <- eigenvec_3[,2] %*% t(eigenvec_3[,2])
mod33 <- eigenvec_3[,3] %*% t(eigenvec_3[,3])

mod81 <- eigenvec_8[,1] %*% t(eigenvec_8[,1])
mod82 <- eigenvec_8[,2] %*% t(eigenvec_8[,2])
mod83 <- eigenvec_8[,3] %*% t(eigenvec_8[,3])

mImage(mod31)
mImage(mod32)
mImage(mod33)
mImage(mod81)
mImage(mod82)
mImage(mod83)

# Il est possible de reconnaitre des 3 et 8 dans les figures

P3 <- eigenvec_3[,1:5]
p3 <- P3 %*% t(P3)
norm(p3 %*% p3 - p3) # C'est bien une matrice de projection

P8 <- eigenvec_8[,1:5]
p8 <- P8 %*% t(P8)
norm(p8 %*% p8 - p8)


# Reconstruction
test_scale <- scale(d3test)

testImg = test_scale[1,]
mImage(testImg + mean3)
projection = t(P3) %*% testImg;
mImage(P3 %*% projection + mean3)

# Ce qui permet de reconstruire les donnÃ©es
# Il suffit donc de garder en mÃ©moire cinq Ã©lÃ©ments ce qui rÃ©duit le stockage